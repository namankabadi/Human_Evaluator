# Human_Evaluator üìù

This tool is designed to assist in the evaluation of automatically generated summaries, particularly focusing on the problem of **relation hallucination** in abstractive summarization. **Relation hallucination** refers to when a model generates summaries that introduce incorrect or irrelevant relations, causing inaccuracies in the generated text.

## Features of the App üöÄ

- **Upload Data**: Users can upload a JSON file containing text and corresponding summaries. The JSON file should follow a specific format, including the original input text, reference summary, and model-generated summaries from multiple models such as **BART**, **Pegasus**, **T5**, **GPT-3.5**, and **RefSum**.

- **Summary Rating**: The app allows users to manually rate the quality of each model's generated summary on a scale from **1 to 10**. The rating takes into account how closely the generated summary aligns with the reference summary, and how accurately it reflects the content of the input text.

- **Evaluation Metrics**: Automatically computed evaluation metrics, such as **cosine similarity**, are used to assess the relevance and quality of summaries. These metrics provide a quantitative score that reflects how well the generated summary matches the reference summary and the input text.

- **Visualization**: The app generates visualizations of the ratings for each model, making it easy to compare the performance of different summarization models. It also shows the frequency distribution of ratings to highlight trends and discrepancies in the evaluation process.

- **Download Results**: After evaluation, users can download the annotated JSON data, which includes the ratings for each model and summary, allowing for further analysis and reporting.

## Data Format üìÅ

The app supports a specific **JSON structure** for easy integration and use. This structure includes fields for the original input text, reference summary, and summaries from various models. By uploading data in the correct format, users can streamline the evaluation process.

**Example JSON Format**:

```json
{
    "0": {
        "Id": "10157432",
        "dataset": "xlsum",
        "InputText": "The full article text goes here...",
        "ReferenceSummary": "The original summary goes here...",
        "facebook/bart-large-cnn": "Model-generated summary here...",
        "google/pegasus-xsum": "Model-generated summary here...",
        "t5-large": "Model-generated summary here...",
        "gpt-3.5-turbo": "Model-generated summary here...",
        "RefSum": "Reference summary for comparison..."
    }
}

# How It Works üõ†Ô∏è

**Input Text:** The full text of the article or passage being summarized.  
**Reference Summary:** The ideal or ground-truth summary provided for comparison.  
**Model Summaries:** Summaries generated by different abstractive summarization models such as:
- **facebook/bart-large-cnn**
- **google/pegasus-xsum**
- **t5-large**
- **gpt-3.5-turbo**

Users can rate these summaries and compare them to the reference summary using the app's features.

## How to Run in Local System:

- Must have **python**
- Must have **streamlit**

**Install streamlit using:**  
`pip install streamlit`

## Key Advantages Include:

- **Manual rating** of summaries based on their alignment with the reference summary.
- **Automatic evaluation** using cosine similarity to quantify the relevance and accuracy of the generated text.
- **Comprehensive visualizations** that help in comparing model performance and rating trends.
- **Downloadable results** for further analysis and reporting.

This tool is ideal for researchers, developers, and evaluators working in the field of **Natural Language Processing (NLP)**, especially in tasks related to **summarization** and the detection of **relation hallucinations**. By providing a clear and structured workflow, it enables better understanding and improvement of summarization models.

## **Conclusion üéØ**

The **Human Evaluator** tool offers an efficient and intuitive approach to evaluating automatically generated summaries, specifically focusing on the problem of **relation hallucination** in abstractive summarization. By allowing users to upload structured JSON data, rate model summaries, and visualize the results, this tool provides a powerful mechanism to assess the performance of various summarization models.
